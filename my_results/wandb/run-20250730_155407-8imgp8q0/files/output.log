Checkpoint loaded successfully from '/var/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/src/models/utae_paps_models/model.pth.tar'
Using the following dataset split:
Train years: [2018, 2019], Val years: [2020], Test years: [2021]
Sanity Checking: 0it [00:00, ?it/s]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
   | Name               | Type                       | Params
-------------------------------------------------------------------
0  | loss               | BCEWithLogitsLoss          | 0
1  | train_f1           | BinaryF1Score              | 0
2  | val_f1             | BinaryF1Score              | 0
3  | test_f1            | BinaryF1Score              | 0
4  | test_avg_precision | BinaryAveragePrecision     | 0
5  | val_avg_precision  | BinaryAveragePrecision     | 0
6  | test_precision     | BinaryPrecision            | 0
7  | test_recall        | BinaryRecall               | 0
8  | test_iou           | BinaryJaccardIndex         | 0
9  | conf_mat           | BinaryConfusionMatrix      | 0
10 | test_pr_curve      | BinaryPrecisionRecallCurve | 0
11 | model              | UTAE                       | 1.1 M
-------------------------------------------------------------------
1.1 M     Trainable params
0         Non-trainable params
1.1 M     Total params


Sanity Checking DataLoader 0:  50%|██████████████████████████████████████                                      | 1/2 [00:03<00:03,  3.81s/it]
/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/.venv/lib64/python3.10/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.

  warnings.warn(*args, **kwargs)  # noqa: B028

Epoch 0:   3%|█▏                                    | 2/62 [00:13<06:52,  6.87s/it, v_num=p8q0, train_loss_step=1.280, train_f1_step=0.00164]
/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/.venv/lib64/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: reflection_pad2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/.venv/lib64/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: upsample_bilinear2d_backward_out_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)


















Epoch 0: 100%|█████████████████████████████████████| 62/62 [00:50<00:00,  1.22it/s, v_num=p8q0, train_loss_step=0.902, train_f1_step=0.00636]































Epoch 1: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.854, train_f1_step=0.0198, val_loss=2.920, val_avg_precision=0.































Epoch 2: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.828, train_f1_step=0.00942, val_loss=2.590, val_avg_precision=0





























Epoch 3: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.827, train_f1_step=0.0076, val_loss=2.130, val_avg_precision=0.
































Epoch 4: 100%|█| 62/62 [00:44<00:00,  1.41it/s, v_num=p8q0, train_loss_step=0.886, train_f1_step=0.0181, val_loss=2.160, val_avg_precision=0.
































Epoch 5: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.878, train_f1_step=0.0246, val_loss=2.180, val_avg_precision=0.






























Epoch 6: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.809, train_f1_step=0.019, val_loss=2.170, val_avg_precision=0.2































Epoch 7: 100%|█| 62/62 [00:42<00:00,  1.46it/s, v_num=p8q0, train_loss_step=0.791, train_f1_step=0.0208, val_loss=2.290, val_avg_precision=0.































Epoch 8: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.868, train_f1_step=0.0357, val_loss=2.080, val_avg_precision=0.
































Epoch 9: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.772, train_f1_step=0.0616, val_loss=2.360, val_avg_precision=0.






























Epoch 10: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.791, train_f1_step=0.064, val_loss=2.000, val_avg_precision=0.































Epoch 11: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.806, train_f1_step=0.0262, val_loss=2.220, val_avg_precision=0






























Epoch 12: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.768, train_f1_step=0.0596, val_loss=2.320, val_avg_precision=0































Epoch 13: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.804, train_f1_step=0.119, val_loss=2.200, val_avg_precision=0.































Epoch 14: 100%|█| 62/62 [00:42<00:00,  1.46it/s, v_num=p8q0, train_loss_step=0.792, train_f1_step=0.067, val_loss=2.340, val_avg_precision=0.






























Epoch 15: 100%|█| 62/62 [00:42<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.815, train_f1_step=0.0488, val_loss=2.280, val_avg_precision=0































Epoch 16: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.771, train_f1_step=0.0429, val_loss=2.450, val_avg_precision=0
































Epoch 17: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.739, train_f1_step=0.0223, val_loss=2.370, val_avg_precision=0































Epoch 18: 100%|█| 62/62 [00:42<00:00,  1.47it/s, v_num=p8q0, train_loss_step=0.737, train_f1_step=0.0523, val_loss=2.350, val_avg_precision=0































Epoch 19: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.756, train_f1_step=0.0769, val_loss=2.320, val_avg_precision=0






























Epoch 20: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.734, train_f1_step=0.0606, val_loss=2.370, val_avg_precision=0































Epoch 21: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.798, train_f1_step=0.0228, val_loss=2.480, val_avg_precision=0































Epoch 22: 100%|█| 62/62 [00:42<00:00,  1.46it/s, v_num=p8q0, train_loss_step=0.762, train_f1_step=0.0352, val_loss=2.330, val_avg_precision=0
































Epoch 23: 100%|█| 62/62 [00:42<00:00,  1.46it/s, v_num=p8q0, train_loss_step=0.768, train_f1_step=0.0409, val_loss=2.520, val_avg_precision=0































Epoch 24: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.783, train_f1_step=0.0367, val_loss=2.590, val_avg_precision=0































Epoch 25: 100%|█| 62/62 [00:42<00:00,  1.46it/s, v_num=p8q0, train_loss_step=0.790, train_f1_step=0.0682, val_loss=2.430, val_avg_precision=0































Epoch 26: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.770, train_f1_step=0.137, val_loss=2.300, val_avg_precision=0.
































Epoch 27: 100%|█| 62/62 [00:42<00:00,  1.47it/s, v_num=p8q0, train_loss_step=0.754, train_f1_step=0.0716, val_loss=2.540, val_avg_precision=0































Epoch 28: 100%|█| 62/62 [00:42<00:00,  1.47it/s, v_num=p8q0, train_loss_step=0.812, train_f1_step=0.104, val_loss=2.400, val_avg_precision=0.































Epoch 29: 100%|█| 62/62 [00:44<00:00,  1.40it/s, v_num=p8q0, train_loss_step=0.789, train_f1_step=0.0609, val_loss=2.480, val_avg_precision=0































Epoch 30: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.772, train_f1_step=0.111, val_loss=2.440, val_avg_precision=0.































Epoch 31: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.740, train_f1_step=0.0338, val_loss=2.440, val_avg_precision=0






























Epoch 32: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.877, train_f1_step=0.171, val_loss=2.490, val_avg_precision=0.
































Epoch 33: 100%|█| 62/62 [00:42<00:00,  1.47it/s, v_num=p8q0, train_loss_step=0.808, train_f1_step=0.0804, val_loss=2.600, val_avg_precision=0






























Epoch 34: 100%|█| 62/62 [00:44<00:00,  1.41it/s, v_num=p8q0, train_loss_step=0.747, train_f1_step=0.0916, val_loss=2.590, val_avg_precision=0






























Epoch 35: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.817, train_f1_step=0.0664, val_loss=2.510, val_avg_precision=0































Epoch 36: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.905, train_f1_step=0.231, val_loss=2.840, val_avg_precision=0.






























Epoch 37: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.746, train_f1_step=0.0313, val_loss=2.490, val_avg_precision=0































Epoch 38: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.819, train_f1_step=0.181, val_loss=2.590, val_avg_precision=0.
































Epoch 39: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.757, train_f1_step=0.0794, val_loss=2.600, val_avg_precision=0































Epoch 40: 100%|█| 62/62 [00:43<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.734, train_f1_step=0.0479, val_loss=2.660, val_avg_precision=0
































Epoch 41: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.862, train_f1_step=0.081, val_loss=2.430, val_avg_precision=0.
































Epoch 42: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.753, train_f1_step=0.104, val_loss=2.480, val_avg_precision=0.































Epoch 43: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.749, train_f1_step=0.0753, val_loss=2.510, val_avg_precision=0
































Epoch 44: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.731, train_f1_step=0.118, val_loss=2.540, val_avg_precision=0.
































Epoch 45: 100%|█| 62/62 [00:43<00:00,  1.42it/s, v_num=p8q0, train_loss_step=0.741, train_f1_step=0.119, val_loss=2.540, val_avg_precision=0.































Epoch 46: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.757, train_f1_step=0.0773, val_loss=2.530, val_avg_precision=0































Epoch 47: 100%|█| 62/62 [00:42<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.782, train_f1_step=0.0987, val_loss=2.620, val_avg_precision=0





























Epoch 48: 100%|█| 62/62 [00:43<00:00,  1.43it/s, v_num=p8q0, train_loss_step=0.745, train_f1_step=0.030, val_loss=2.410, val_avg_precision=0.































Epoch 49: 100%|█| 62/62 [00:42<00:00,  1.44it/s, v_num=p8q0, train_loss_step=0.735, train_f1_step=0.0999, val_loss=2.570, val_avg_precision=0
































Epoch 50: 100%|█| 62/62 [00:42<00:00,  1.45it/s, v_num=p8q0, train_loss_step=0.743, train_f1_step=0.0388, val_loss=2.620, val_avg_precision=0










Train years: [2018, 2019], Val years: [2020], Test years: [2021]███████████████████████████████▎             | 43/52 [00:18<00:03,  2.34it/s]
/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/.venv/lib64/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Restoring states from the checkpoint path at /var/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/my_results/wildfire_progression/8imgp8q0/checkpoints/epoch=17-step=1116.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /var/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/my_results/wildfire_progression/8imgp8q0/checkpoints/epoch=17-step=1116.ckpt
Testing: 46it [00:01, 24.30it/s]
/home/lornjaeger/distrobox/homes/dev/projects/WildfireSpreadTS/.venv/lib64/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples found in target, recall is undefined. Setting recall to one for all thresholds.









































































































Testing: 3316it [03:33, 15.51it/s]
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
         test_AP            0.2875106930732727
         test_f1            0.1162090077996254
        test_iou            0.06168890744447708
        test_loss           1.5894345045089722
     test_precision         0.06196971610188484
       test_recall           0.931570827960968
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────